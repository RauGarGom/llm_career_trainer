{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Let's get started with a technical question related to Data Science.\n",
      "\n",
      "**Question:**\n",
      "Can you explain the concept of overfitting in machine learning and how you can mitigate it?\n",
      "\n",
      "Feel free to provide a detailed explanation along with code snippets or examples if you'd like.\n",
      "That's a good start, but let's refine the explanation slightly for precision. Overfitting occurs when a model learns the training data too well, capturing noise and details specific to the training dataset that do not generalize to new, unseen data. This often results in high accuracy on the training set but poor performance on the validation or test set.\n",
      "\n",
      "To better understand this concept, let's delve into a specific question: \n",
      "\n",
      "**Question:** How can you detect if a model is overfitting, and what are some methods to mitigate overfitting without reducing the model's ability to learn from the data?\n",
      "\n",
      "Feel free to explain your thoughts on this!\n",
      "Great starting point! Let's dig a bit deeper into these concepts.\n",
      "\n",
      "**Question:** Could you explain how L1 and L2 regularization work, and how they differ? Could you also provide an example of when you might prefer one over the other in a practical scenario?\n",
      "\n",
      "Go ahead and try to explain this in more detail.\n",
      "Certainly! Let's dive into a technical question to evaluate your understanding of data science concepts. Hereâ€™s your first question:\n",
      "\n",
      "**Question:**\n",
      "Can you explain the difference between supervised and unsupervised learning with examples?\n",
      "\n",
      "Please go ahead and share your thoughts on this topic.\n",
      "Your explanation of supervised and unsupervised learning is quite good, but it could benefit from a bit more detail and clarity. Let's go ahead and refine this understanding a bit further.\n",
      "\n",
      "**Rephrased Explanation:**\n",
      "\n",
      "- **Supervised Learning:** In supervised learning, the model is trained on a labeled dataset, which means it has input-output pairs. The goal is to learn a mapping from the input to the output. For example, predicting future weather conditions based on historical data, where the historical data includes both the weather conditions (inputs) and the outcomes (outputs).\n",
      "\n",
      "- **Unsupervised Learning:** In unsupervised learning, the model is trained on an unlabeled dataset, meaning it only has input data and no target output to predict. The aim is to infer the natural structure present within a set of data points. Examples include clustering customer groups based on purchasing behavior or discovering patterns in customer feedback.\n",
      "\n",
      "Now, considering this clarification, let's move on to a more specific technical question related to supervised learning:\n",
      "\n",
      "**Question:** \n",
      "\"Can you describe a common algorithm used in supervised learning and explain how it works? Feel free to use an example for clarity.\"\n",
      "\n",
      "Please respond to the question and we can proceed from there.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"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\")\n",
    "\n",
    "messages = [\n",
    "\t{\"role\": \"system\", \"content\": \"You are a technical career interviewer. The user is an interviewee looking for a Data Science position. Ask them a first techical question, evaluate their response, and proceed to a next one once you consider the answer is good enough \"},\n",
    "\t# {\"role\": \"user\",\"content\": \"What's the first question?\",\n",
    "\t# }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "i=1\n",
    "while i < 5:\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"system\", \"content\": \"You are a technical career interviewer. The user is an interviewee looking for a Data Science position. Ask them a first techical question, evaluate their response, and proceed to a next one once you consider the answer is good enough \"},\n",
    "\t\t{\"role\": \"user\",\"content\": input('write your answer here'),\n",
    "\t\t} \n",
    "\t] #\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\t\tmessages=messages, \n",
    "\t\tmax_tokens=500\n",
    "\t)\n",
    "\tprint(completion.choices[0].message.content)\n",
    "\ti+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "try:\n",
    "    mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "except:\n",
    "    mistral_api_key = getpass.getpass('Insert Mistral API Key')\n",
    "try:\n",
    "    langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "except:\n",
    "    langchain_api_key = getpass.getpass('Insert Langchain API Key')\n",
    "    \n",
    "# gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a short technical question for you:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please take a moment to think about it and provide your answer when you're ready.\n",
      "Sure, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it. When you're ready for the next question, just let me know!\n",
      "Sure, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between a Type I and a Type II error in hypothesis testing?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it.\n",
      "Alright, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it before moving on to the next question if you'd like.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    SystemMessage(\"You are an interviewer for a Data Science position. Ask one short, random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\"),\n",
    "    # HumanMessage(\"What is the question?\"),\n",
    "]\n",
    "response = model.invoke(message)\n",
    "print(response.content)\n",
    "i = 0\n",
    "while i < 3:\n",
    "    message = [\n",
    "        HumanMessage(input('answer here')),\n",
    "        SystemMessage(\"You are an interviewer for a Data Science position. Ask one short, random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\")\n",
    "    ]\n",
    "    response = model.invoke(message)\n",
    "    print(response.content)\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\",temperature=0.9)\n",
    "\n",
    "system_template = \"You are an interviewer for a Data Science position. Evaluate the answer of {answer} and make a follow-up question, but don't explain it. Be concise.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{answer}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the concept of overfitting in machine learning? How would you detect and prevent it?\n",
      "\n",
      "Please take your time to provide a detailed answer.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    SystemMessage(\"You are an interviewer for a Data Science position. Ask one random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\")\n",
    "    # HumanMessage(\"What is the question?\"),\n",
    "]\n",
    "response = model.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Evaluation:** The candidate provided a clear and concise definition of overfitting, highlighting the issue of poor generalization to unknown data. They also correctly identified two common techniques to address overfittingâ€”regularization and cross-validation. Overall, the answer is accurate and demonstrates a basic understanding of the concept.\n",
      "\n",
      "**Follow-up Question:**\n",
      "Can you briefly explain how regularization helps in mitigating overfitting?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"answer\": input('answer here')})\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's start with a technical question related to Data Science.\n",
      "\n",
      "**Question:**\n",
      "Can you explain the concept of Overfitting and Underfitting in the context of machine learning models?\n",
      "\n",
      "Please provide your answer, and we'll evaluate it together.\n",
      "I am answering a question on time series analysis. I was just asked \"Can you explain the applications of the ARIMA model in time series forecasting?\"\n",
      "\n",
      "I would answer it as \"ARIMA, or AutoRegressive Integrated Moving Average, is a popular model for time series forecasting. It captures temporal dependencies in the data by combining autoregression, differencing, and moving average components. Applications include financial forecasting, such as predicting stock prices or sales forecasting and revenue prediction.\n",
      "\n",
      "Follow-up question: Can you describe a situation where ARIMA might fail or be less effective?\"\n"
     ]
    }
   ],
   "source": [
    "import llm_career_trainer.model as md\n",
    "md.interview_training_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing prompt behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import model as md\n",
    "import pandas as pd\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\",temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_questions = ['How do you determine if a dataset is biased or unbalanced, and how would you address it?',\n",
    "                'Explain the concept of overfitting and underfitting in machine learning.',\n",
    "                'How would you test if two variables are statistically independent?',\n",
    "                'Given a dataset with missing values, what techniques would you use to handle them? When would you use imputation over deletion?',\n",
    "                'How would you detect and handle outliers in a dataset?',\n",
    "                'Explain the difference between normalization and standardization. When would you use each?',\n",
    "                'Describe how you would approach feature engineering for a dataset with categorical and numerical features.',\n",
    "                'Explain the difference between bagging and boosting. Provide examples of algorithms for each.',\n",
    "                'How would you evaluate the performance of a classification model? Which metrics would you choose and why?',\n",
    "                'You have an imbalanced dataset. What strategies would you apply to train a robust machine learning model?',\n",
    "                'Describe the bias-variance tradeoff and its impact on model performance.',\n",
    "                'What is the purpose of regularization in machine learning? Compare L1 (Lasso) and L2 (Ridge) regularization.',\n",
    "                'Write a Python function to calculate the precision, recall, and F1-score for a binary classification problem.',\n",
    "                'How would you optimize the performance of a machine learning pipeline in Python? Mention libraries or tools you would use.',\n",
    "                'What are the differences between pandas and numpy? When would you use each?',\n",
    "                'Explain the role of the learning rate in training neural networks. How do you decide on an appropriate learning rate, and what are the consequences of setting it too high or too low?',\n",
    "                'In ensemble learning, how does a Random Forest differ from Gradient Boosted Trees (e.g., XGBoost, LightGBM)? Which situations favor each?',\n",
    "                'How would you handle a scenario where your model is accurate overall but performs poorly for a specific subset of your data?',\n",
    "                'What is transfer learning, and how can it be applied to reduce training time in a machine learning project? Provide a practical example of its usage.',\n",
    "                \"Can you explain the difference between supervised and unsupervised learning?\",\n",
    "                \"How would you handle missing data in a dataset?\",\n",
    "                \"What is the bias-variance tradeoff, and how do you balance it?\",\n",
    "                \"Describe a situation where you had to use feature engineering. What was the outcome?\",\n",
    "                \"How do you evaluate the performance of a classification model?\",\n",
    "                \"What is cross-validation, and why is it important?\",\n",
    "                \"Explain the concept of overfitting and how you can prevent it.\",\n",
    "                \"How do you handle imbalanced datasets?\",\n",
    "                \"What is the difference between a parametric and a non-parametric model?\",\n",
    "                \"Can you explain the concept of dimensionality reduction and provide an example?\"\n",
    "                ]\n",
    "questions_dict = {'question':ds_questions}\n",
    "df_questions = pd.DataFrame(questions_dict)\n",
    "# df_questions.to_csv('./data/questions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a concise evaluation framework for assessing an interviewee\\'s understanding of overfitting:\\n\\n### Evaluation Criteria:\\n\\n1. **Definition and Explanation**:\\n   - **Excellent**: The interviewee clearly defines overfitting as the situation where a model performs well on training data but poorly on unseen data. They explain that this happens when the model learns the noise and details in the training data, rather than the underlying pattern.\\n   - **Good**: The interviewee provides a basic definition of overfitting and mentions that it occurs when the model doesn\\'t generalize well to new data.\\n   - **Fair**: The interviewee has a vague understanding of overfitting, mentioning it as a problem but without a clear explanation.\\n   - **Poor**: The interviewee is unable to define overfitting or provides an incorrect explanation.\\n\\n2. **Causes of Overfitting**:\\n   - **Excellent**: The interviewee identifies multiple causes such as high model complexity, too many features, insufficient training data, and noise in the training data.\\n   - **Good**: The interviewee mentions at least one or two causes of overfitting.\\n   - **Fair**: The interviewee provides a general idea but lacks specific details.\\n   - **Poor**: The interviewee is unable to identify any causes of overfitting.\\n\\n3. **Solutions to Overfitting**:\\n   - **Excellent**: The interviewee suggests multiple solutions such as cross-validation, regularization (L1, L2), pruning, dropout, early stopping, and collecting more data.\\n   - **Good**: The interviewee mentions one or two solutions like regularization or cross-validation.\\n   - **Fair**: The interviewee provides a general idea but lacks specific techniques.\\n   - **Poor**: The interviewee is unable to suggest any solutions to overfitting.\\n\\n4. **Practical Example**:\\n   - **Excellent**: The interviewee provides a clear, practical example of overfitting, such as a decision tree model that perfectly fits the training data but fails on the test data.\\n   - **Good**: The interviewee gives a general example but lacks detail.\\n   - **Fair**: The interviewee provides a vague example.\\n   - **Poor**: The interviewee is unable to provide any example.\\n\\n### Sample Answer Evaluation:\\n\\n**Interviewee Answer**:\\n\"Overfitting happens when a model learns the training data too well, including the noise and outliers, which leads to poor performance on new, unseen data. This can be caused by having too many features relative to the number of observations, or by using a model that is too complex for the data. To mitigate overfitting, we can use techniques like cross-validation, regularization, or collecting more data.\"\\n\\n**Evaluation**:\\n- **Definition and Explanation**: Good\\n- **Causes of Overfitting**: Good\\n- **Solutions to Overfitting**: Good\\n- **Practical Example**: Fair (no specific example provided)\\n\\n**Overall Assessment**: Good\\n\\nThis framework can help you quickly evaluate an interviewee\\'s understanding of overfitting in a structured and concise manner.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"You're a technical Data Science interviewer. Evaluate the knwoledge of the interviewee by their answer on {topic}. Be concise\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({\"topic\": \"Overfitting happens when the trained model doesn't generalize correctly\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"Make a following question based on {reasoning}. Just give the question and be concise.\")\n",
    "\n",
    "composed_chain = {\"reasoning\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "follow_up = composed_chain.invoke({\"topic\": \"Overfitting happens when the trained model doesn't generalize correctly\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you explain what overfitting is, its common causes, how to identify it, and techniques to prevent it? Additionally, can you provide an example of overfitting in a specific machine learning context?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConexiÃ³n a postgre AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConexiÃ³n exitosa a PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import model as md\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ParÃ¡metros de conexiÃ³n\n",
    "host = \"llm-db.c9egi4wa8bqm.eu-west-1.rds.amazonaws.com\"\n",
    "username = \"postgres\"\n",
    "password = os.getenv(\"POSTGRE_PASS\")\n",
    "port = 5432\n",
    "\n",
    "\n",
    "# ConexiÃ³n a PostgreSQL\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    cursor_factory=DictCursor  # Devuelve filas como diccionarios\n",
    ")\n",
    "print(\"ConexiÃ³n exitosa a PostgreSQL\")\n",
    "\n",
    "# Crear un cursor\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table = '''DROP TABLE regist;'''\n",
    "cursor.execute(drop_table)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = '''\n",
    "CREATE TABLE regist (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    action TEXT,\n",
    "    llm_user TEXT,\n",
    "    content TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "'''\n",
    "cursor.execute(create_table)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedObject",
     "evalue": "unrecognized configuration parameter \"tables\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedObject\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSHOW TABLES\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cursor\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\psycopg2\\extras.py:146\u001b[0m, in \u001b[0;36mDictCursor.execute\u001b[1;34m(self, query, vars)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_executed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUndefinedObject\u001b[0m: unrecognized configuration parameter \"tables\"\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SHOW TABLES')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data = '''\n",
    "INSERT INTO regist (action,llm_user,content)\n",
    "VALUES ('testing','test_user','test from work journal')\n",
    "'''\n",
    "cursor.execute(insert_data)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a technical question for you:\\n\\n\"Can you explain the difference between L1 and L2 regularization, and provide an example of when you might use each?\"\\n\\nPlease take your time to think about it and provide a detailed answer.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.generate_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Candidate\\'s Answer:**\\n\"In my previous role, I used Python and R for data analysis and predictive modeling. I worked on a project where I had to predict customer churn for a telecom company. I started by cleaning the data using Pandas, then performed exploratory data analysis with Matplotlib and Seaborn. I tried different algorithms like Logistic Regression, Random Forest, and XGBoost. The best performance was achieved with XGBoost, with an 85% accuracy rate. I also used grid search for hyperparameter tuning. The results were interpreted and presented to the stakeholders using PowerBI.\"\\n\\n**Evaluation:** Good understanding of the data science workflow, familiarity with essential tools and libraries, and experience with predictive modeling.\\n\\n**Follow-up Question:**\\nCan you describe a situation where you had to deal with imbalanced data? How did you handle it?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.evaluate_answer(\"test from work journal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

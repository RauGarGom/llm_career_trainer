{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "POST 401 Insufficient balance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m\n\u001b[0;32m      8\u001b[0m api_request_json \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.1-70b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_current_weather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m }\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Execute the Request\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_request_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(response\u001b[38;5;241m.\u001b[39mjson(), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llamaapi\\llamaapi.py:67\u001b[0m, in \u001b[0;36mLlamaAPI.run\u001b[1;34m(self, api_request_json)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_stream(api_request_json)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_request_json\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llamaapi\\llamaapi.py:53\u001b[0m, in \u001b[0;36mLlamaAPI.run_sync\u001b[1;34m(self, api_request_json)\u001b[0m\n\u001b[0;32m     51\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhostname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mapi_request_json)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:        \n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mException\u001b[0m: POST 401 Insufficient balance."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "# Initialize the SDK\n",
    "llama = LlamaAPI(\"LA-5dd18cd76d8b498292e8d7ef0650ffee3c51f937f44141f195f6d0f474d35949\")\n",
    "\n",
    "# Build the API request\n",
    "api_request_json = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"days\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"for how many days ahead you wants the forecast\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"days\"],\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"function_call\": \"get_current_weather\",\n",
    "}\n",
    "\n",
    "# Execute the Request\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForCausalLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-Coder-32B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m      6\u001b[0m     model_name,\n\u001b[0;32m      7\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich is your question?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1630\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForCausalLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Which is your question?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a technical career interviewer. The user is an interviewee looking for a Data Science position. Ask them technical questions and evaluate their responses \"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Let's get started with a technical question related to Data Science.\n",
      "\n",
      "**Question:**\n",
      "Can you explain the concept of overfitting in machine learning and how you can mitigate it?\n",
      "\n",
      "Feel free to provide a detailed explanation along with code snippets or examples if you'd like.\n",
      "That's a good start, but let's refine the explanation slightly for precision. Overfitting occurs when a model learns the training data too well, capturing noise and details specific to the training dataset that do not generalize to new, unseen data. This often results in high accuracy on the training set but poor performance on the validation or test set.\n",
      "\n",
      "To better understand this concept, let's delve into a specific question: \n",
      "\n",
      "**Question:** How can you detect if a model is overfitting, and what are some methods to mitigate overfitting without reducing the model's ability to learn from the data?\n",
      "\n",
      "Feel free to explain your thoughts on this!\n",
      "Great starting point! Let's dig a bit deeper into these concepts.\n",
      "\n",
      "**Question:** Could you explain how L1 and L2 regularization work, and how they differ? Could you also provide an example of when you might prefer one over the other in a practical scenario?\n",
      "\n",
      "Go ahead and try to explain this in more detail.\n",
      "Certainly! Let's dive into a technical question to evaluate your understanding of data science concepts. Here’s your first question:\n",
      "\n",
      "**Question:**\n",
      "Can you explain the difference between supervised and unsupervised learning with examples?\n",
      "\n",
      "Please go ahead and share your thoughts on this topic.\n",
      "Your explanation of supervised and unsupervised learning is quite good, but it could benefit from a bit more detail and clarity. Let's go ahead and refine this understanding a bit further.\n",
      "\n",
      "**Rephrased Explanation:**\n",
      "\n",
      "- **Supervised Learning:** In supervised learning, the model is trained on a labeled dataset, which means it has input-output pairs. The goal is to learn a mapping from the input to the output. For example, predicting future weather conditions based on historical data, where the historical data includes both the weather conditions (inputs) and the outcomes (outputs).\n",
      "\n",
      "- **Unsupervised Learning:** In unsupervised learning, the model is trained on an unlabeled dataset, meaning it only has input data and no target output to predict. The aim is to infer the natural structure present within a set of data points. Examples include clustering customer groups based on purchasing behavior or discovering patterns in customer feedback.\n",
      "\n",
      "Now, considering this clarification, let's move on to a more specific technical question related to supervised learning:\n",
      "\n",
      "**Question:** \n",
      "\"Can you describe a common algorithm used in supervised learning and explain how it works? Feel free to use an example for clarity.\"\n",
      "\n",
      "Please respond to the question and we can proceed from there.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"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\")\n",
    "\n",
    "messages = [\n",
    "\t{\"role\": \"system\", \"content\": \"You are a technical career interviewer. The user is an interviewee looking for a Data Science position. Ask them a first techical question, evaluate their response, and proceed to a next one once you consider the answer is good enough \"},\n",
    "\t# {\"role\": \"user\",\"content\": \"What's the first question?\",\n",
    "\t# }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "i=1\n",
    "while i < 5:\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"system\", \"content\": \"You are a technical career interviewer. The user is an interviewee looking for a Data Science position. Ask them a first techical question, evaluate their response, and proceed to a next one once you consider the answer is good enough \"},\n",
    "\t\t{\"role\": \"user\",\"content\": input('write your answer here'),\n",
    "\t\t} \n",
    "\t] #\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "\t\tmessages=messages, \n",
    "\t\tmax_tokens=500\n",
    "\t)\n",
    "\tprint(completion.choices[0].message.content)\n",
    "\ti+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a short technical question for you:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please take a moment to think about it and provide your answer when you're ready.\n",
      "Sure, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it. When you're ready for the next question, just let me know!\n",
      "Sure, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between a Type I and a Type II error in hypothesis testing?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it.\n",
      "Alright, let's start with a technical question:\n",
      "\n",
      "**Question:** Can you explain the difference between L1 and L2 regularization?\n",
      "\n",
      "Please provide your answer, and I'll evaluate it before moving on to the next question if you'd like.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    SystemMessage(\"You are an interviewer for a Data Science position. Ask one short, random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\"),\n",
    "    # HumanMessage(\"What is the question?\"),\n",
    "]\n",
    "response = model.invoke(message)\n",
    "print(response.content)\n",
    "i = 0\n",
    "while i < 3:\n",
    "    message = [\n",
    "        HumanMessage(input('answer here')),\n",
    "        SystemMessage(\"You are an interviewer for a Data Science position. Ask one short, random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\")\n",
    "    ]\n",
    "    response = model.invoke(message)\n",
    "    print(response.content)\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "system_template = \"You are an interviewer for a Data Science position. Evaluate the answer of {answer} and make a follow-up question, but don't explain it. Be concise.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{answer}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's begin with a technical question related to data science.\n",
      "\n",
      "**Question:**\n",
      "Can you explain the concept of overfitting in machine learning and provide an example of how you might address it?\n",
      "\n",
      "Please take your time to answer, and let me know when you're ready for the next question.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    SystemMessage(\"You are an interviewer for a Data Science position. Ask one random technical question and evaluate the answers of the user. Change to another data science question when they ask to.\")\n",
    "    # HumanMessage(\"What is the question?\"),\n",
    "]\n",
    "response = model.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Evaluation:** The candidate provided a clear and concise definition of overfitting, highlighting the issue of poor generalization to unknown data. They also correctly identified two common techniques to address overfitting—regularization and cross-validation. Overall, the answer is accurate and demonstrates a basic understanding of the concept.\n",
      "\n",
      "**Follow-up Question:**\n",
      "Can you briefly explain how regularization helps in mitigating overfitting?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"answer\": input('answer here')})\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's start with a technical question related to Data Science.\n",
      "\n",
      "**Question:**\n",
      "Can you explain the concept of Overfitting and Underfitting in the context of machine learning models?\n",
      "\n",
      "Please provide your answer, and we'll evaluate it together.\n"
     ]
    }
   ],
   "source": [
    "import backend.model as md\n",
    "md.interview_training_local()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
